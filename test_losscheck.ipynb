{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6e0750-9588-4bba-ae23-c75dd2a38a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4c8ee-2af6-4cb2-94ab-27f55bea0140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183bceaf-815f-44e1-b8f1-4fe42879bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "import math\n",
    "\n",
    "def compute_loss_gp( pred_mu,pred_std, target_y , intrain=True):\n",
    "\n",
    "    \"\"\"\n",
    "    compute NLLLossLNPF\n",
    "    # computes approximate LL in a numerically stable way\n",
    "    # LL = E_{q(z|y_cntxt)}[ \\prod_t p(y^t|z)]\n",
    "    # LL MC = log ( mean_z ( \\prod_t p(y^t|z)) )\n",
    "    # = log [ sum_z ( \\prod_t p(y^t|z)) ] - log(n_z_samples)\n",
    "    # = log [ sum_z ( exp \\sum_t log p(y^t|z)) ] - log(n_z_samples)\n",
    "    # = log_sum_exp_z ( \\sum_t log p(y^t|z)) - log(n_z_samples)\n",
    "    \n",
    "    \"\"\"\n",
    "    #(nsamples,nb,ndata,nchannels) \n",
    "    p_yCc = Normal(loc=pred_mu, scale=pred_std)                \n",
    "    \n",
    "    if intrain:\n",
    "        #(numsamples,nb) \n",
    "        sumlogprob = p_yCc.log_prob(target_y).sum(dim=(-1,-2))   #sum over channels and targets    \n",
    "        logmeanexp_sumlogprob= torch.logsumexp(sumlogprob, dim=0) -  math.log(sumlogprob.size(0)) \n",
    "\n",
    "        #meanlogprob = p_yCc.log_prob(target_y).mean(dim=(-1,-2))     #mean over channels and targets \n",
    "        #logmeanexp_sumlogprob= torch.logsumexp(meanlogprob, dim=0) -  math.log(meanlogprob.size(0))     \n",
    "    else :\n",
    "        #(numsamples,nb) \n",
    "        #sumlogprob = p_yCc.log_prob(target_y).sum(dim=(-1,-2))   #sum over channels and targets    \n",
    "        #logmeanexp_sumlogprob= torch.logsumexp(sumlogprob, dim=0) -  math.log(sumlogprob.size(0)) \n",
    "\n",
    "        meanlogprob = p_yCc.log_prob(target_y).mean(dim=(-1,-2))     #mean over channels and targets \n",
    "        logmeanexp_sumlogprob= torch.logsumexp(meanlogprob, dim=0) -  math.log(meanlogprob.size(0))     \n",
    "        \n",
    "    return -logmeanexp_sumlogprob.mean() #mean over batches\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "eps=1e-8\n",
    "def compute_loss_gp_origin( pred_mu,pred_std, target_y, z_samples=None, qz_c=None, qz_ct=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    compute NLLLossLNPF\n",
    "    # computes approximate LL in a numerically stable way\n",
    "    # LL = E_{q(z|y_cntxt)}[ \\prod_t p(y^t|z)]\n",
    "    # LL MC = log ( mean_z ( \\prod_t p(y^t|z)) )\n",
    "    # = log [ sum_z ( \\prod_t p(y^t|z)) ] - log(n_z_samples)\n",
    "    # = log [ sum_z ( exp \\sum_t log p(y^t|z)) ] - log(n_z_samples)\n",
    "    # = log_sum_exp_z ( \\sum_t log p(y^t|z)) - log(n_z_samples)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def sum_from_nth_dim(t, dim):\n",
    "        \"\"\"Sum all dims from `dim`. E.g. sum_after_nth_dim(torch.rand(2,3,4,5), 2).shape = [2,3]\"\"\"\n",
    "        return t.view(*t.shape[:dim], -1).sum(-1)\n",
    "\n",
    "\n",
    "    def sum_log_prob(prob, sample):\n",
    "        \"\"\"Compute log probability then sum all but the z_samples and batch.\"\"\"    \n",
    "        log_p = prob.log_prob(sample)          # size = [n_z_samples, batch_size, *]    \n",
    "        sum_log_p = sum_from_nth_dim(log_p, 2) # size = [n_z_samples, batch_size]\n",
    "        return sum_log_p\n",
    "\n",
    "    \n",
    "    p_yCc = Normal(loc=pred_mu, scale=pred_std)    \n",
    "    if qz_c is not None:\n",
    "        qz_c = Normal(loc=qz_c[0], scale=qz_c[1])\n",
    "        \n",
    "    if qz_ct is not None:\n",
    "        qz_ct = Normal(loc=qz_ct[0], scale=qz_ct[1])\n",
    "        \n",
    "        \n",
    "    n_z_samples, batch_size, *n_trgt = p_yCc.batch_shape    \n",
    "    # \\sum_t log p(y^t|z). size = [n_z_samples, batch_size]\n",
    "    sum_log_p_yCz = sum_log_prob(p_yCc, target_y)\n",
    "\n",
    "    \n",
    "    # uses importance sampling weights if necessary\n",
    "    if z_samples is not None:\n",
    "        # All latents are treated as independent. size = [n_z_samples, batch_size]\n",
    "        sum_log_qz_c = sum_log_prob(qz_c, z_samples)\n",
    "        sum_log_qz_ct = sum_log_prob(qz_ct, z_samples)\n",
    "        # importance sampling : multiply \\prod_t p(y^t|z)) by q(z|y_cntxt) / q(z|y_cntxt, y_trgt)\n",
    "        # i.e. add log q(z|y_cntxt) - log q(z|y_cntxt, y_trgt)\n",
    "        #print(sum_log_p_yCz, sum_log_qz_c, sum_log_qz_ct)\n",
    "        sum_log_w_k = sum_log_p_yCz + sum_log_qz_c - sum_log_qz_ct\n",
    "    else:\n",
    "        sum_log_w_k = sum_log_p_yCz\n",
    "\n",
    "    # log_sum_exp_z ... . size = [batch_size]\n",
    "    log_S_z_sum_p_yCz = torch.logsumexp(sum_log_w_k + eps, 0)\n",
    "    # - log(n_z_samples)\n",
    "    log_E_z_sum_p_yCz = log_S_z_sum_p_yCz - math.log(n_z_samples)    \n",
    "\n",
    "    #print('log_E_z_sum_p_yCz {}'.format(log_E_z_sum_p_yCz.mean().item()))\n",
    "    # NEGATIVE log likelihood\n",
    "    #return -log_E_z_sum_p_yCz\n",
    "    return -log_E_z_sum_p_yCz.mean()  #averages each loss over batches \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6094f99f-20d9-4989-bc20-7240a5fd162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(nsamples,nbatch,ndata,nchannel)\n",
    "pmu = 0.5*torch.randn(5,16,30,3)\n",
    "pstd = 0.1*torch.rand(5,16,30,3)\n",
    "yobs = torch.randn(16,30,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "022cee31-681a-45ae-a197-89faaff60e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(275620.8438), tensor(275620.8438))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_gp_origin( pmu,pstd, yobs),   compute_loss_gp( pmu,pstd, yobs , intrain=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6393c4f-e345-4454-9ad1-f389e0864a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36567445-efd5-4613-aae7-4ffe65f5818c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
